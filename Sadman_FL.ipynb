{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5e4b849",
   "metadata": {},
   "source": [
    "source: https://github.com/SadmanSakib93/Federated-Learning-Keras/blob/main/Fed%20Learning%20-%20FL.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41766872",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-15 17:00:51.998476: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-15 17:00:52.124306: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-11-15 17:00:52.128896: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-11-15 17:00:52.128913: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-11-15 17:00:52.151041: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-11-15 17:00:52.602710: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-15 17:00:52.602763: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-15 17:00:52.602768: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.utils import np_utils\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten, BatchNormalization\n",
    "from tensorflow.keras.layers import Convolution2D, Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling2D, MaxPooling1D\n",
    "from keras import backend as K\n",
    "from keras import backend\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import time\n",
    "import os\n",
    "import psutil\n",
    "import csv\n",
    "from itertools import repeat\n",
    "from PIL import Image\n",
    "from numpy import asarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7062dd7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dab0150d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfDS = pd.read_csv('~/Desktop/MedBiot/medbiot_original/fine-grained/raw_dataset/fine_grained_csv/csv_modified/device_data/mod_combined_2000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f73095c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   frame_len frame_protocols  ip_len    ip_flags  ip_ttl  ip_proto  \\\n",
      "0         66             tcp      52  0x00004000      64         6   \n",
      "1       1514             tcp    1500  0x00004000      64         6   \n",
      "2         74             tcp      60  0x00004000      64         6   \n",
      "3         74             tcp      60  0x00004000      64         6   \n",
      "4         67          telnet      53  0x00004000      64         6   \n",
      "\n",
      "           ip_src          ip_dst  tcp_srcport  tcp_dstport  ...  \\\n",
      "0   192.168.10.61  192.168.10.199      47518.0         23.0  ...   \n",
      "1   192.168.10.61  192.168.10.192         80.0      35464.0  ...   \n",
      "2  192.168.10.190    221.41.1.240      53714.0         23.0  ...   \n",
      "3  192.168.10.196  136.51.183.121      58538.0         23.0  ...   \n",
      "4   192.168.10.61  192.168.10.189         23.0      48132.0  ...   \n",
      "\n",
      "   tcp_window_size  tcp_window_size_scalefactor tcp_time_relative  \\\n",
      "0          43136.0                        128.0          0.746154   \n",
      "1          30080.0                        128.0          0.026004   \n",
      "2          29200.0                          NaN          3.134363   \n",
      "3          29200.0                          NaN          7.231050   \n",
      "4          29056.0                        128.0       8866.809236   \n",
      "\n",
      "   tcp_time_delta  tcp_analysis_bytes_in_flight  tcp_analysis_push_bytes_sent  \\\n",
      "0        0.001162                           NaN                           NaN   \n",
      "1        0.000000                        2896.0                       37648.0   \n",
      "2        2.078187                           NaN                           NaN   \n",
      "3        4.083804                           NaN                           NaN   \n",
      "4        0.004394                           1.0                           1.0   \n",
      "\n",
      "   is_malware  malware_type  device   phase  \n",
      "0           1      bashlite    lock      cc  \n",
      "1           1      bashlite    lock      cc  \n",
      "2           1      bashlite    lock  spread  \n",
      "3           1      bashlite    lock  spread  \n",
      "4           1      bashlite    lock      cc  \n",
      "\n",
      "[5 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "print(dfDS.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d47f84e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputx = ['frame_len', 'ip_len', 'ip_ttl', 'ip_proto', 'tcp_srcport', 'tcp_dstport']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "947ebdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_full = dfDS.iloc[:, 1:len(dfDS.columns)].values\n",
    "x = dfDS.loc[:, inputx].values\n",
    "y = dfDS['is_malware'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b655dbb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.6000e+01 5.2000e+01 6.4000e+01 6.0000e+00 4.7518e+04 2.3000e+01]\n",
      " [1.5140e+03 1.5000e+03 6.4000e+01 6.0000e+00 8.0000e+01 3.5464e+04]\n",
      " [7.4000e+01 6.0000e+01 6.4000e+01 6.0000e+00 5.3714e+04 2.3000e+01]\n",
      " ...\n",
      " [6.8000e+01 5.4000e+01 6.4000e+01 6.0000e+00 5.3333e+04 1.8830e+03]\n",
      " [7.4000e+01 6.0000e+01 6.4000e+01 6.0000e+00 4.0540e+04 8.0000e+01]\n",
      " [6.0000e+01 4.0000e+01 6.4000e+01 6.0000e+00 8.0000e+01 5.9076e+04]]\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e5e4854f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "x = scaler.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dc1cefc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.12654746e-03 8.21917808e-03 2.48031496e-01 1.08695652e-01\n",
      "  7.79005525e-01 3.60738530e-04]\n",
      " [1.00000000e+00 1.00000000e+00 2.48031496e-01 1.08695652e-01\n",
      "  1.29514566e-03 5.81494113e-01]\n",
      " [9.62861073e-03 1.36986301e-02 2.48031496e-01 1.08695652e-01\n",
      "  8.80584291e-01 3.60738530e-04]\n",
      " ...\n",
      " [5.50206327e-03 9.58904110e-03 2.48031496e-01 1.08695652e-01\n",
      "  8.74338082e-01 3.08595415e-02]\n",
      " [9.62861073e-03 1.36986301e-02 2.48031496e-01 1.08695652e-01\n",
      "  6.64606456e-01 1.29537927e-03]\n",
      " [0.00000000e+00 0.00000000e+00 2.48031496e-01 1.08695652e-01\n",
      "  1.29514566e-03 9.68664939e-01]]\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2919d326",
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrain, xTest, yTrain, yTest = train_test_split(x, y, test_size=0.10, random_state=123) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b2b99e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.50206327e-03 9.58904110e-03 2.48031496e-01 1.08695652e-01\n",
      "  8.70108366e-01 3.08595415e-02]\n",
      " [5.50206327e-03 9.58904110e-03 2.48031496e-01 1.08695652e-01\n",
      "  6.52359952e-01 3.08595415e-02]\n",
      " [5.50206327e-03 9.58904110e-03 2.44094488e-01 1.08695652e-01\n",
      "  3.08539764e-02 9.51070738e-01]\n",
      " ...\n",
      " [4.12654746e-03 8.21917808e-03 2.48031496e-01 1.08695652e-01\n",
      "  9.98163844e-01 3.60738530e-04]\n",
      " [9.72489684e-01 9.72602740e-01 2.08661417e-01 1.08695652e-01\n",
      "  1.29514566e-03 7.17951661e-01]\n",
      " [9.62861073e-03 1.36986301e-02 2.48031496e-01 1.08695652e-01\n",
      "  6.09193895e-01 3.60738530e-04]]\n"
     ]
    }
   ],
   "source": [
    "print(xTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5f1325fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xTrain (21600, 6, 1) yTrain (21600, 2, 2, 2)\n",
      "xTest (2400, 6, 1) yTest (2400, 2, 2, 2)\n"
     ]
    }
   ],
   "source": [
    "algoName= 'DNN'#'CNN' #CNN, ANN, DNN\n",
    "\n",
    "xTrain = xTrain.astype('float32')\n",
    "xTest = xTest.astype('float32')\n",
    "xTrain = xTrain / 255.\n",
    "xTest = xTest / 255.\n",
    "\n",
    "if(algoName=='CNN'):\n",
    "    xTrain = np.expand_dims(xTrain, axis=2)\n",
    "    xTest = np.expand_dims(xTest, axis=2)\n",
    "\n",
    "outputClasses=len(set(Y_full))\n",
    "#One hot encoding\n",
    "yTrain = np.array(to_categorical(yTrain))\n",
    "yTest = np.array(to_categorical(yTest))\n",
    "print(\"xTrain\", xTrain.shape, \"yTrain\", yTrain.shape)\n",
    "print(\"xTest\", xTest.shape, \"yTest\", yTest.shape)\n",
    "\n",
    "# FOR TEST SPLIT\n",
    "xServer, xClients, yServer, yClients = train_test_split(xTrain, yTrain, test_size=0.80,random_state=523) \n",
    "\n",
    "def my_metrics(y_true, y_pred):\n",
    "    accuracy=accuracy_score(y_true, y_pred)\n",
    "    precision=precision_score(y_true, y_pred,average='weighted')\n",
    "    recall=recall_score(y_true, y_pred,average='weighted')\n",
    "    f1Score=f1_score(y_true, y_pred, average='weighted') \n",
    "    print(\"Accuracy  : {}\".format(accuracy))\n",
    "    print(\"Precision : {}\".format(precision))\n",
    "    print(\"Recall : {}\".format(recall))\n",
    "    print(\"f1Score : {}\".format(f1Score))\n",
    "    cm=confusion_matrix(y_true, y_pred)\n",
    "    print(cm)\n",
    "    return accuracy, precision, recall, f1Score\n",
    "\n",
    "verbose, epochs, batch_size = 0, 20, 64\n",
    "activationFun='relu'\n",
    "optimizerName='Adam'\n",
    "def createDeepModel():\n",
    "    model = Sequential()\n",
    "    \n",
    "    if(algoName=='CNN'):    \n",
    "        model.add(Conv1D(filters=10, kernel_size=3, activation=activationFun,input_shape = (X_full.shape[1], 1)))\n",
    "        model.add(Dropout(0.1))\n",
    "        model.add(MaxPooling1D(pool_size=3))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        model.add(Conv1D(filters=5, kernel_size=3, activation=activationFun))\n",
    "        model.add(Dropout(0.05))\n",
    "        model.add(MaxPooling1D(pool_size=3))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(100, activation=activationFun))\n",
    "        model.add(Dense(50, activation=activationFun))\n",
    "        model.add(Dense(30, activation=activationFun))\n",
    "        model.add(Dense(outputClasses, activation='softmax'))\n",
    "        model.compile(loss='mean_squared_error', optimizer=optimizerName, metrics=['accuracy'])\n",
    "        \n",
    "    elif(algoName=='ANN'):\n",
    "        model.add(Dense(200, input_dim=X_full.shape[1], activation=activationFun))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(outputClasses, activation='softmax'))\n",
    "        model.compile(loss='mean_squared_error', optimizer=optimizerName, metrics=['accuracy'])\n",
    "        \n",
    "    elif(algoName=='DNN'):\n",
    "        model.add(Dense(200, input_dim=X_full.shape[1], activation=activationFun))\n",
    "        model.add(Dense(100, activation=activationFun))\n",
    "        model.add(Dense(50, activation=activationFun))\n",
    "        model.add(Dense(25,  activation=activationFun))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(outputClasses, activation='softmax'))\n",
    "        model.compile(loss='binary_crossentropy', optimizer=optimizerName, metrics=['accuracy'])\n",
    "        \n",
    "        \n",
    "    return model\n",
    "\n",
    "def predictTestData(yPredict, yTest):\n",
    "    #Converting predictions to label\n",
    "    print(\"yPredict\",len(yPredict))\n",
    "    pred = list()\n",
    "    for i in range(len(yPredict)):\n",
    "        pred.append(np.argmax(yPredict[i]))\n",
    "    #Converting one hot encoded test label to label\n",
    "    test = list()\n",
    "    for i in range(len(yTest)):\n",
    "        test.append(np.argmax(yTest[i]))\n",
    "    return my_metrics(test, pred)\n",
    "\n",
    "def sumOfWeights(weights):\n",
    "    return sum(map(sum, weights))\n",
    "\n",
    "def getWeights(model):\n",
    "    allLayersWeights=deepModel.get_weights()\n",
    "    return allLayersWeights\n",
    "    \n",
    "# Initially train central deep model\n",
    "deepModel=createDeepModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0e0200c9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type int).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [32]\u001b[0m, in \u001b[0;36m<cell line: 31>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m     deepModel\u001b[38;5;241m.\u001b[39mfit(X_full, Y_full, epochs\u001b[38;5;241m=\u001b[39mepochs, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, verbose\u001b[38;5;241m=\u001b[39mverbose)\n\u001b[1;32m     30\u001b[0m     deepModel\u001b[38;5;241m.\u001b[39msave(modelLocation)\n\u001b[0;32m---> 31\u001b[0m \u001b[43mtrainInServer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# ------- 2. Separate clients data into lists ----------\u001b[39;00m\n\u001b[1;32m     33\u001b[0m xClientsList\u001b[38;5;241m=\u001b[39m[]\n",
      "Input \u001b[0;32mIn [32]\u001b[0m, in \u001b[0;36mtrainInServer\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrainInServer\u001b[39m():\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m#deepModel.fit(xServer, yServer, epochs=epochs, batch_size=batch_size, verbose=verbose)\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m     \u001b[43mdeepModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_full\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_full\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m     deepModel\u001b[38;5;241m.\u001b[39msave(modelLocation)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py:102\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    100\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[1;32m    101\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m--> 102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type int)."
     ]
    }
   ],
   "source": [
    "numOfIterations=50\n",
    "numOfClients=10 # 10, 15, 20, 25, 30, 35, 40, 45, 50\n",
    "modelLocation=\"Models/\"+str(algoName)+\"_Sync_users_\"+str(numOfClients)+\"_\"+activationFun+\"_\"+optimizerName+\"_FL_Model.h5\"\n",
    "accList, precList, recallList, f1List = [], [], [], []\n",
    "\n",
    "deepModelAggWeights=[]\n",
    "firstClientFlag=True\n",
    "\n",
    "def updateServerModel(clientModel, clientModelWeight):\n",
    "    global firstClientFlag\n",
    "    for ind in range(len(clientModelWeight)):\n",
    "        if(firstClientFlag==True):\n",
    "            deepModelAggWeights.append(clientModelWeight[ind])            \n",
    "        else:\n",
    "            deepModelAggWeights[ind]=(deepModelAggWeights[ind]+clientModelWeight[ind])\n",
    "\n",
    "def updateClientsModels():\n",
    "    global clientsModelList\n",
    "    global deepModel\n",
    "    clientsModelList.clear()\n",
    "    for clientID in range(numOfClients):\n",
    "        m = keras.models.clone_model(deepModel)\n",
    "        m.set_weights(deepModel.get_weights())\n",
    "        clientsModelList.append(m)\n",
    "    \n",
    "# ----- 1. Train central model initially -----\n",
    "def trainInServer():\n",
    "    deepModel.fit(xServer, yServer, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "    # deepModel.fit(X_full, Y_full, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "    deepModel.save(modelLocation)\n",
    "trainInServer()\n",
    "# ------- 2. Separate clients data into lists ----------\n",
    "xClientsList=[]\n",
    "yClientsList=[]\n",
    "clientsModelList=[]\n",
    "clientDataInterval=len(xClients)//numOfClients\n",
    "lastLowerBound=0\n",
    "\n",
    "for clientID in range(numOfClients):\n",
    "    xClientsList.append(xClients[lastLowerBound : lastLowerBound+clientDataInterval])\n",
    "    yClientsList.append(yClients[lastLowerBound : lastLowerBound+clientDataInterval])\n",
    "    model=load_model(modelLocation)\n",
    "    clientsModelList.append(model)\n",
    "    lastLowerBound+=clientDataInterval\n",
    "# ------- 3. Update clients' model with intial server's deep-model ----------\n",
    "for clientID in range(numOfClients):\n",
    "    clientsModelList[clientID].fit(xClientsList[clientID], yClientsList[clientID], epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "        \n",
    "start_time = time.time()\n",
    "process = psutil.Process(os.getpid())\n",
    "for iterationNo in range(1,numOfIterations+1):\n",
    "    print(\"Iteration\",iterationNo)\n",
    "    for clientID in range(numOfClients):\n",
    "        print(\"clientID\",clientID)\n",
    "        clientsModelList[clientID].compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "        clientsModelList[clientID].fit(xClientsList[clientID], yClientsList[clientID], epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "        clientWeight=clientsModelList[clientID].get_weights()\n",
    "        # Find sum of all client's model\n",
    "        updateServerModel(clientsModelList[clientID], clientWeight)\n",
    "        firstClientFlag=False\n",
    "    #Avarage all clients model\n",
    "    for ind in range(len(deepModelAggWeights)):\n",
    "        deepModelAggWeights[ind]/=numOfClients\n",
    "\n",
    "    dw_last=deepModel.get_weights()\n",
    "\n",
    "    for ind in range(len(deepModelAggWeights)): \n",
    "        dw_last[ind]=deepModelAggWeights[ind]\n",
    "     \n",
    "    #Update server's model\n",
    "    deepModel.set_weights(dw_last) \n",
    "    print(\"Server's model updated\")\n",
    "    print(\"Saving model . . .\")\n",
    "    deepModel.save(modelLocation)\n",
    "    # Servers model is updated, now it can be used again by the clients\n",
    "    updateClientsModels()\n",
    "    firstClientFlag=True\n",
    "    deepModelAggWeights.clear()\n",
    "\n",
    "    yPredict = deepModel.predict(xTest)\n",
    "    acc, prec, recall, f1Score= predictTestData(yPredict, yTest)\n",
    "    accList.append(acc)\n",
    "    precList.append(prec)\n",
    "    recallList.append(recall)\n",
    "    f1List.append(f1Score)\n",
    "    print(\"Acc:\\n\", acc)\n",
    "    print(\"Prec:\\n\", prec)\n",
    "    print(\"Recall:\\n\", recall)\n",
    "    print(\"F1-Score:\\n\", f1Score)\n",
    "\n",
    "memoryTraining=process.memory_percent()\n",
    "timeTraining=time.time() - start_time\n",
    "print(\"---Memory---\",memoryTraining)\n",
    "print(\"--- %s seconds (TRAINING)---\" % (timeTraining))\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=0, mode='auto')\n",
    "\n",
    "history = deepModel.fit(xServer, yServer, epochs=epochs, \n",
    "                        validation_data = (xTest,yTest))\n",
    "                        # callbacks=[early_stopping])\n",
    "\n",
    "learningAccs=history.history['val_accuracy']\n",
    "learningLoss=history.history['val_loss']\n",
    "\n",
    "# resultSaveLocation=root_path+'Results/'+algoName+'_Users_vs_TR_vs_Iterations_vs_AccLossMemTime'+'.csv'\n",
    "dfSave=pd.DataFrame(columns=['Clients', 'Iterations to converge', 'Accuracy', 'Loss', 'Memory', 'Time'])\n",
    "dfSaveIndex=0\n",
    "saveList = [numOfClients, len(learningLoss), learningAccs[len(learningAccs)-1], learningLoss[len(learningLoss)-1], memoryTraining, timeTraining]\n",
    "dfSave.loc[dfSaveIndex] = saveList\n",
    "\n",
    "yPredict = deepModel.predict(xTest)\n",
    "acc, prec, recall, f1Score= predictTestData(yPredict, yTest)\n",
    "\n",
    "print(\"Number of users:\", numOfClients)\n",
    "deepModel.save(modelLocation)\n",
    "print(\"Epochs:\", epochs)\n",
    "print(\"BatchSize:\", batch_size)\n",
    "print(\"Activation:\", activationFun, \"Optimizer:\", optimizerName)\n",
    "\n",
    "print(\"Iterations:\", numOfIterations)\n",
    "print(\"Memory:\", memoryTraining)\n",
    "print(\"Time:\", timeTraining)\n",
    "print(dfSave)\n",
    "\n",
    "df_performance_timeRounds = pd.DataFrame(\n",
    "    {'Accuracy': accList,\n",
    "     'Precision': precList,\n",
    "     'Recall': recallList,\n",
    "     'F1-Score': f1List \n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0386048e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf]",
   "language": "python",
   "name": "conda-env-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
